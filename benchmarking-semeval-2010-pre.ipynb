{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9266ff",
   "metadata": {},
   "source": [
    "# Load a spacy model with in-words hyphens fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb58a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'textcat', 'parser'])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Tokenization fix for in-word hyphens (e.g. 'non-linear' would be kept \n",
    "# as one token instead of default spacy behavior of 'non', '-', 'linear')\n",
    "# https://spacy.io/usage/linguistic-features#native-tokenizer-additions\n",
    "\n",
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Modify tokenizer infix patterns\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        # âœ… Commented out regex that splits on hyphens between letters:\n",
    "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531efc2",
   "metadata": {},
   "source": [
    "# Pre-process train and test splits using spacy and serialize docbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9e3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sem_eval/raw\n",
      "Reusing dataset sem_eval (/Users/boudinfl/.cache/huggingface/datasets/taln-ls2n___sem_eval/raw/1.0.0/b40e008b5c96137733e24d9d244d70aa1fe6353ee65e180d8f6948af4027fbe4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87204f175dac494faf18607cdc26abdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# load the inspec dataset\n",
    "benchmark = \"semeval-2010-pre\"\n",
    "dataset = load_dataset('taln-ls2n/semeval-2010-pre')\n",
    "\n",
    "# pre-process training and test splits\n",
    "for split in ['train', 'test']:\n",
    "    output_file = \"data/{}.{}.docbin\".format(benchmark, split)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    doc_bin = DocBin()\n",
    "    for sample in tqdm(dataset[split]):\n",
    "        doc = nlp(sample[\"title\"]+\". \"+sample[\"abstract\"])\n",
    "        doc_bin.add(doc)\n",
    "    bytes_data = doc_bin.to_bytes()\n",
    "    with open(output_file, 'wb') as o:\n",
    "        o.write(bytes_data)\n",
    "    del doc_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504c882",
   "metadata": {},
   "source": [
    "# Compute DF counts (TfIdf), LDA model (TopicalPageRank) and Kea model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e727ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pke import compute_document_frequency, compute_lda_model\n",
    "from string import punctuation\n",
    "\n",
    "df_file = \"data/{}.df.gz\".format(benchmark)\n",
    "train_file = \"data/{}.train.docbin\".format(benchmark)\n",
    "train = []\n",
    "\n",
    "if not os.path.exists(df_file):\n",
    "    if not len(train):\n",
    "        with open(train_file, 'rb') as f: \n",
    "            doc_bin = DocBin().from_bytes(f.read())\n",
    "            train = list(doc_bin.get_docs(nlp.vocab))\n",
    "    \n",
    "    compute_document_frequency(\n",
    "        documents=train,\n",
    "        output_file=df_file, \n",
    "        language='en',                   # language of the input files\n",
    "        normalization='stemming',        # use porter stemmer\n",
    "        stoplist=list(punctuation),      # stoplist (punctuation marks)\n",
    "        n=5                              # compute n-grams up to 5-grams\n",
    "    )\n",
    "\n",
    "lda_file = \"data/{}.lda.pickle.gz\".format(benchmark)\n",
    "if not os.path.exists(lda_file):\n",
    "    if not len(train):\n",
    "        with open(train_file, 'rb') as f: \n",
    "            doc_bin = DocBin().from_bytes(f.read())\n",
    "            train = list(doc_bin.get_docs(nlp.vocab))\n",
    "            \n",
    "    compute_lda_model(\n",
    "        documents=train,\n",
    "        output_file=lda_file,\n",
    "        n_topics=1000,              # number of topics\n",
    "        language='en',              # language of the input files\n",
    "        stoplist=list(punctuation), # stoplist (punctuation marks)\n",
    "        normalization='stemming'    # use porter stemmer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084fe9a",
   "metadata": {},
   "source": [
    "# Load DF counts and LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7853dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pke import load_document_frequency_file, load_lda_model\n",
    "\n",
    "df = load_document_frequency_file(input_file=df_file)\n",
    "lda_model = load_lda_model(input_file=lda_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac46a1c",
   "metadata": {},
   "source": [
    "# Train a Kea model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530d5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "from pke import train_supervised_model\n",
    "from nltk.stem.snowball import SnowballStemmer as Stemmer\n",
    "\n",
    "kea_file = \"data/{}.kea.model.pickle\".format(benchmark)\n",
    "if not os.path.exists(kea_file):\n",
    "    if not len(train):\n",
    "        with open(train_file, 'rb') as f: \n",
    "            doc_bin = DocBin().from_bytes(f.read())\n",
    "            train = list(doc_bin.get_docs(nlp.vocab))\n",
    "        \n",
    "        samples = []\n",
    "        references = {}\n",
    "        for sample in tqdm(dataset[\"train\"]):\n",
    "            samples.append((sample[\"id\"], train[len(samples)]))\n",
    "            references[sample[\"id\"]] = sample[\"keyphrases\"]\n",
    "            #for keyphrase in sample[\"keyphrases\"]:\n",
    "            #    tokens = [token.text for token in nlp(keyphrase)]\n",
    "            #    stems = [Stemmer('porter').stem(tok.lower()) for tok in tokens]\n",
    "            #    references[sample[\"id\"]].append(\" \".join(stems))\n",
    "        \n",
    "        train_supervised_model(\n",
    "            documents=samples,\n",
    "            references=references,\n",
    "            model_file=kea_file,\n",
    "            language='en',\n",
    "            normalization='stemming',\n",
    "            df=df,\n",
    "            model=pke.supervised.Kea()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4f6ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa46f9c17283492984144e5d4bbdc9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699681ffdec94611b73ac37bc0e9fad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3fd2f24beb42d092787db080524ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1e8681678142aa854d65f48a2e2a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb9466da6334be1895c7d75ae693ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1084e0b3e74e4a9cccfdf889915c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6b7b65151f41eb95c3b6f9689960ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e33fe56cdc4bbd80bfebb37046d1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boudinfl/Documents/GitHub/pke-benchmarking/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919c56b425c46d9bab51dd3859e2e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0d38d55c8d4cb98c9d652216930946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f589cec8124d18a3a56fc2183e342e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pke.unsupervised import *\n",
    "from pke.supervised import *\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "test_file = \"data/{}.test.docbin\".format(benchmark)\n",
    "with open(test_file, 'rb') as f: \n",
    "    doc_bin = DocBin().from_bytes(f.read())\n",
    "    test = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "outputs = {}\n",
    "elapsed_times = {}\n",
    "for model in [FirstPhrases, TfIdf, KPMiner, YAKE, TextRank, SingleRank, TopicRank, TopicalPageRank, PositionRank, MultipartiteRank,    Kea]:\n",
    "    outputs[model.__name__] = []\n",
    "    \n",
    "    extractor = model()\n",
    "    start = timer()\n",
    "    for i, doc in enumerate(tqdm(test)):\n",
    "        extractor.load_document(input=doc, language='en')\n",
    "        extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "        if model.__name__ in [\"TfIdf\", \"KPMiner\"]:\n",
    "            extractor.candidate_weighting(df=df)\n",
    "        elif model.__name__ in [\"Kea\"]:\n",
    "            extractor.candidate_weighting(df=df, model_file=kea_file)\n",
    "        elif model.__name__ in [\"TopicalPageRank\"]:\n",
    "            extractor.candidate_weighting(lda_model=lda_model)\n",
    "        elif model.__name__ in [\"TextRank\", \"SingleRank\", \"PositionRank\", \"TopicalPageRank\"]:\n",
    "            extractor.candidate_weighting(normalized=False)\n",
    "        else:\n",
    "            extractor.candidate_weighting()\n",
    "        outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=50, stemming=True)])\n",
    "    end = timer()\n",
    "    #print(model.__name__, outputs[model.__name__][0])\n",
    "    elapsed_times[model.__name__] = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1249a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381e086afc0f40e6996e9f97de89d68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer as Stemmer\n",
    "import numpy as np\n",
    "    \n",
    "# populates the references list with stemmed keyphrases\n",
    "references = []\n",
    "for sample in tqdm(dataset['test']):\n",
    "    references.append(sample[\"keyphrases\"])\n",
    "    #sample_keyphrases = []\n",
    "    #for keyphrase in sample[\"keyphrases\"]:\n",
    "    #    tokens = [token.text for token in nlp(keyphrase)]\n",
    "    #    stems = [Stemmer('porter').stem(tok.lower()) for tok in tokens]\n",
    "    #    sample_keyphrases.append(\" \".join(stems))\n",
    "    #references.append(sample_keyphrases)\n",
    "\n",
    "def evaluate(top_N_keyphrases, references, cutoff=5):\n",
    "    P = len(set(top_N_keyphrases[:cutoff]) & set(references)) / len(top_N_keyphrases[:cutoff])\n",
    "    R = len(set(top_N_keyphrases[:cutoff]) & set(references)) / len(references)\n",
    "    F = (2*P*R)/(P+R) if (P+R) > 0 else 0\n",
    "    avg_P = 0.0\n",
    "    for i, k in enumerate(top_N_keyphrases):\n",
    "        if k in references:\n",
    "            avg_P += len(set(top_N_keyphrases[:i+1]) & set(references)) / (i+1)\n",
    "    avg_P = avg_P / len(references)\n",
    "    return (P, R, F, avg_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a7d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Benchmarking on semeval-2010-pre\n",
      "| Model | it/s |  F@5 | F@10 |  mAP |\n",
      "| :---- | ----:| ---: | ---: | ---: |\n",
      "| FirstPhrases  | 204 | 14.0 | 15.5 | 10.8 |\n",
      "| TfIdf  | 177 | 13.0 | 16.4 | 11.2 |\n",
      "| KPMiner  | 173 | 12.4 | 17.2 | 10.9 |\n",
      "| YAKE  | 7 | 16.0 | 18.4 | 11.5 |\n",
      "| TextRank  | 115 | 9.9 | 13.9 | 8.3 |\n",
      "| SingleRank  | 126 | 11.9 | 16.7 | 9.7 |\n",
      "| TopicRank  | 85 | 12.1 | 14.8 | 8.4 |\n",
      "| TopicalPageRank  | 25 | 12.0 | 16.7 | 10.1 |\n",
      "| PositionRank  | 124 | 12.9 | 17.6 | 10.7 |\n",
      "| MultipartiteRank  | 65 | 14.0 | 15.8 | 11.1 |\n",
      "| Kea  | 163 | 14.2 | 17.3 | 11.7 |\n"
     ]
    }
   ],
   "source": [
    "print(\"## Benchmarking on {}\".format(benchmark))\n",
    "print(\"| Model | it/s |  F@5 | F@10 |  mAP |\")\n",
    "print(\"| :---- | ----:| ---: | ---: | ---: |\")\n",
    "\n",
    "# loop through the models\n",
    "for model in outputs:\n",
    "    \n",
    "    f_scores = []\n",
    "    \n",
    "    # compute the P, R, F scores for the model\n",
    "    for cutoff in [5, 10]:\n",
    "        scores = []\n",
    "        for i, output in enumerate(outputs[model]):\n",
    "            scores.append(evaluate(output, references[i], cutoff))\n",
    "        # compute the average scores\n",
    "        P, R, F, mAP = np.mean(scores, axis=0)\n",
    "        f_scores.append(F)\n",
    "        f_scores.append(mAP)\n",
    "    #print(f_scores)\n",
    "        \n",
    "    print(\"| {}  | {:.0f} | {:.1f} | {:.1f} | {:.1f} |\".format(model,  len(test)/ elapsed_times[model], f_scores[0]*100, f_scores[2]*100, f_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841bd1d",
   "metadata": {},
   "source": [
    "## Benchmarking on semeval-2010-pre\n",
    "| Model | it/s |  F@5 | F@10 |  mAP |\n",
    "| :---- | ----:| ---: | ---: | ---: |\n",
    "| FirstPhrases  | 202 | 14.0 | 15.5 | 10.8 |\n",
    "| TfIdf  | 198 | 13.0 | 16.4 | 11.2 |\n",
    "| KPMiner  | 197 | 12.4 | 17.2 | 10.9 |\n",
    "| YAKE  | 7 | 16.0 | 18.4 | 11.5 |\n",
    "| TextRank  | 129 | 9.9 | 13.9 | 8.3 |\n",
    "| SingleRank  | 133 | 11.9 | 16.7 | 9.7 |\n",
    "| TopicRank  | 85 | 12.1 | 14.8 | 8.4 |\n",
    "| TopicalPageRank  | 28 | 12.0 | 16.7 | 10.1 |\n",
    "| PositionRank  | 129 | 12.9 | 17.6 | 10.7 |\n",
    "| MultipartiteRank  | 60 | 14.0 | 15.8 | 11.1 |\n",
    "| Kea  | 159 | 14.2 | 17.3 | 11.7 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
